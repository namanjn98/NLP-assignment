{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Assignment - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Parse the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Sherlock Holmes\n"
     ]
    }
   ],
   "source": [
    "txt = open('sher.txt')\n",
    "txt = txt.read()\n",
    "print 'Text: Sherlock Holmes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = RegexpTokenizer(r'[\\w\\.\\?\\!\\'-]+') \n",
    "test = token.tokenize(txt)\n",
    "\n",
    "ban = []\n",
    "can__ = []\n",
    "can_ = []\n",
    "\n",
    "for i,w in enumerate(test):\n",
    "    if '.' in w:\n",
    "        if len(w) > 1 and w[-2] in 'IXV':\n",
    "            ban.append(i)  \n",
    "    if \"'\" in w:\n",
    "        test[i] = test[i].replace(\"'\",'')\n",
    "            \n",
    "for i,w in enumerate(test):            \n",
    "    if '--' in w:\n",
    "        test[i] = test[i].replace('--',' ')\n",
    "    elif '-' in w:\n",
    "        test[i] = test[i].replace('-',' ')\n",
    "\n",
    "t = []\n",
    "for j in range(len(test)):\n",
    "    if j in ban:\n",
    "        continue\n",
    "    else:\n",
    "        t.append(test[j])\n",
    "\n",
    "test = t\n",
    "test = ' '.join(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.lower()\n",
    "txt_sent = sent_tokenize(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(txt_sent)):\n",
    "    txt_sent[i] = '<s> '+ txt_sent[i][:-1] + ' </s>'  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = len(txt_sent)*8/10\n",
    "train = txt_sent[:index]\n",
    "test = txt_sent[index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams & MLE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.probability import MLEProbDist\n",
    "from nltk.probability import ConditionalFreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = ' '.join(txt_sent)\n",
    "words = txt.split(' ')\n",
    "\n",
    "for i in words:\n",
    "    try:\n",
    "        words.remove('')\n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unigram exists = 7992\n",
      "Number of unigrams possible = 7992\n",
      "\n",
      "Compute MLE using MLE_unigram(word)\n",
      "\n",
      "Example:\n",
      "MLE_unigram(\"the\") = 0.047362\n",
      "MLE_unigram(\"holmes\") = 0.003887\n"
     ]
    }
   ],
   "source": [
    "cfdist_uni = FreqDist(words)\n",
    "\n",
    "def MLE_unigram(a):\n",
    "    k = MLEProbDist(cfdist_uni)\n",
    "    return k.prob(a)\n",
    "\n",
    "count_uni = cfdist_uni.B()\n",
    "print \"Number of unigram exists = %d\"%(count_uni)\n",
    "print \"Number of unigrams possible = %d\"%(count_uni)\n",
    "\n",
    "print '\\nCompute MLE using MLE_unigram(word)\\n'\n",
    "print 'Example:'\n",
    "print 'MLE_unigram(\"the\") = %f'%(MLE_unigram(\"the\"))\n",
    "print 'MLE_unigram(\"holmes\") = %f'%(MLE_unigram(\"holmes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bigram exists = 49315\n",
      "Number of bigrams possible = 31932036\n",
      "\n",
      "Compute MLE using MLE_bigram(word)\n",
      "\n",
      "Example:\n",
      "MLE_bigram(\"is\",\"the\") = 0.065836\n",
      "MLE_bigram(\"said\",\"holmes\") = 0.226337\n"
     ]
    }
   ],
   "source": [
    "cfdist_bi = ConditionalFreqDist()\n",
    "for i,w in enumerate(words[:-1]):\n",
    "    condition = w\n",
    "    if w == '</s>':\n",
    "        continue\n",
    "    else:\n",
    "        cfdist_bi[condition][words[i+1]] += 1\n",
    "\n",
    "def MLE_bigram(a,b):\n",
    "    k = MLEProbDist(cfdist_bi[a])\n",
    "    return k.prob(b)\n",
    "\n",
    "count_bi=0\n",
    "for key in cfdist_bi:\n",
    "    count_bi += cfdist_bi[key].B()\n",
    "\n",
    "print \"Number of bigram exists = %d\"%(count_bi)\n",
    "\n",
    "all_count_bi = (count_uni/2) * (count_uni - 1)\n",
    "print \"Number of bigrams possible = %d\"%(all_count_bi)\n",
    "\n",
    "print '\\nCompute MLE using MLE_bigram(word)\\n'\n",
    "print 'Example:'\n",
    "print 'MLE_bigram(\"is\",\"the\") = %f'%(MLE_bigram(\"is\",\"the\"))\n",
    "print 'MLE_bigram(\"said\",\"holmes\") = %f'%(MLE_bigram(\"said\",\"holmes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trigram exists = 84956\n",
      "Number of trigrams possible = 85045655880\n",
      "\n",
      "Compute MLE using MLE_trigram(word)\n",
      "\n",
      "Example:\n",
      "MLE_trigram(\"i\",\"have\",\"a\") = 0.043189\n",
      "MLE_trigram(\"go\",\"for\",\"it\") = 0.200000\n"
     ]
    }
   ],
   "source": [
    "cfdist_tri = ConditionalFreqDist()\n",
    "for i,w in enumerate(words[:-2]):\n",
    "    condition = words[i] +' ' + words[i+1]\n",
    "    if '</s>' in condition:\n",
    "        continue\n",
    "    else:\n",
    "        cfdist_tri[condition][words[i+2]] += 1\n",
    "\n",
    "def MLE_trigram(a,b,c):\n",
    "    k = MLEProbDist(cfdist_tri[a+\" \"+b])\n",
    "    return k.prob(c)\n",
    "\n",
    "count_tri=0\n",
    "for key in cfdist_tri:\n",
    "    count_tri += cfdist_tri[key].B()\n",
    "\n",
    "print \"Number of trigram exists = %d\"%(count_tri)\n",
    "\n",
    "all_count_tri = (count_uni/6) * (count_uni - 1) *  (count_uni - 2)\n",
    "print \"Number of trigrams possible = %d\"%(all_count_tri)\n",
    "\n",
    "print '\\nCompute MLE using MLE_trigram(word)\\n'\n",
    "print 'Example:'\n",
    "print 'MLE_trigram(\"i\",\"have\",\"a\") = %f'%(MLE_trigram(\"i\",\"have\",\"a\"))\n",
    "print 'MLE_trigram(\"go\",\"for\",\"it\") = %f'%(MLE_trigram(\"go\",\"for\",\"it\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of quadgram exists = 93648\n",
      "Number of quadgrams possible = 169857436206330\n",
      "\n",
      "Compute MLE using MLE_quadgram(word)\n",
      "\n",
      "Example:\n",
      "MLE_quadgram(\"i\",\"know\",\"that\",\"i\") = 0.200000\n",
      "MLE_quadgram(\"he\",\"drew\",\"out\",\"a\") = 1.000000\n"
     ]
    }
   ],
   "source": [
    "cfdist_quad = ConditionalFreqDist()\n",
    "for i,w in enumerate(words[:-3]):\n",
    "    condition = words[i] +' ' + words[i+1] + ' ' + words[i+2]\n",
    "    if '</s>' in condition:\n",
    "        continue\n",
    "    else:\n",
    "        cfdist_quad[condition][words[i+3]] += 1\n",
    "\n",
    "def MLE_quadgram(a,b,c,d):\n",
    "    k = MLEProbDist(cfdist_quad[a+\" \"+b+\" \"+c])\n",
    "    return k.prob(d)\n",
    "\n",
    "count_quad = 0\n",
    "for key in cfdist_quad:\n",
    "    count_quad += cfdist_quad[key].B()\n",
    "\n",
    "print \"Number of quadgram exists = %d\"%(count_quad)\n",
    "\n",
    "all_count_quad = (count_uni/24) * (count_uni - 1) * (count_uni - 2) *  (count_uni - 3)\n",
    "print \"Number of quadgrams possible = %d\"%(all_count_quad)\n",
    "\n",
    "print '\\nCompute MLE using MLE_quadgram(word)\\n'\n",
    "print 'Example:'\n",
    "print 'MLE_quadgram(\"i\",\"know\",\"that\",\"i\") = %f'%(MLE_quadgram(\"i\",\"know\",\"that\",\"i\"))\n",
    "print 'MLE_quadgram(\"he\",\"drew\",\"out\",\"a\") = %f'%(MLE_quadgram(\"he\",\"drew\",\"out\",\"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator(model):       #input is a model from [1,2,3,4]\n",
    "    if model == 1:         \n",
    "        sent = ['<s>']\n",
    "        i = 0\n",
    "        while i == 0:\n",
    "            arr = cfdist_uni.keys()\n",
    "            prob = [MLE_unigram(key) for key in arr]\n",
    "            \n",
    "            gen_word = np.random.choice(arr,1,p = prob)   #randomly selecting words with probability\n",
    "            \n",
    "            if gen_word[0] != '<s>':      #Avoiding '<s>' words\n",
    "                sent.append(gen_word[0])\n",
    "                \n",
    "            if gen_word == '</s>':\n",
    "                i = 1\n",
    "        \n",
    "    elif model == 2:\n",
    "        sent = ['<s>']\n",
    "        i = 0\n",
    "        while i == 0:\n",
    "            arr = cfdist_bi[sent[-1]].keys()\n",
    "            prob = [MLE_bigram(sent[-1],key) for key in arr]\n",
    "            gen_word = np.random.choice(arr,1,p = prob)\n",
    "            sent.append(gen_word[0])\n",
    "            if gen_word == '</s>':\n",
    "                i = 1\n",
    "        \n",
    "    elif model == 3:\n",
    "        sent = ['<s>']      #First word after '<s>' can be selected via bi-gram as P(<s>,<s>,FW) == P(<s>,FW)\n",
    "        \n",
    "        arr = cfdist_bi[sent[-1]].keys()\n",
    "        prob = [MLE_bigram(sent[-1],key) for key in arr]\n",
    "        gen_word = np.random.choice(arr,1,p = prob)\n",
    "        sent.append(gen_word[0])\n",
    "        \n",
    "        i = 0\n",
    "        while i == 0:\n",
    "            arr = cfdist_tri[sent[-2] +' '+ sent[-1]].keys()\n",
    "            prob = [MLE_trigram(sent[-2],sent[-1],key) for key in arr]\n",
    "            gen_word = np.random.choice(arr,1,p = prob)\n",
    "            sent.append(gen_word[0])\n",
    "            if gen_word == '</s>':\n",
    "                i = 1\n",
    "        \n",
    "    elif model == 4:\n",
    "        sent = ['<s>']      #First word after '<s>' can be selected via bi-gram as P(<s>,<s>,<s>,FW) == P(<s>,FW)\n",
    "        \n",
    "        arr = cfdist_bi[sent[-1]].keys()\n",
    "        prob = [MLE_bigram(sent[-1],key) for key in arr]\n",
    "        gen_word = np.random.choice(arr,1,p = prob)\n",
    "        sent.append(gen_word[0])\n",
    "        \n",
    "        i = 0\n",
    "        while i == 0:       #Second word after '<s>' can be selected via tri-gram as P(<s>,<s>,FW,SW) == P(<s>,FW,SW)\n",
    "            arr = cfdist_tri[sent[-2] +' '+ sent[-1]].keys()\n",
    "            prob = [MLE_trigram(sent[-2],sent[-1],key) for key in arr]\n",
    "            gen_word = np.random.choice(arr,1,p = prob)\n",
    "            i = 1\n",
    "            \n",
    "            if '</s>' in gen_word[0]:\n",
    "                i = 0\n",
    "            else:\n",
    "                sent.append(gen_word[0])\n",
    "        \n",
    "        i = 0\n",
    "        while i == 0:\n",
    "            arr = cfdist_quad[sent[-3]+' '+sent[-2] +' '+ sent[-1]].keys()\n",
    "            prob = [MLE_quadgram(sent[-3],sent[-2],sent[-1],key) for key in arr]\n",
    "            gen_word = np.random.choice(arr,1,p = prob)\n",
    "            sent.append(gen_word[0])\n",
    "            if gen_word == '</s>':\n",
    "                i = 1\n",
    "    \n",
    "    sent = ' '.join(sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example for 1-gram model\n",
      "\n",
      "<s> held so that at very the who the see mr. that we clean you no determined him compunction that what until with his should tree i and now of royal may right line jack clear sure of i his business my your my i much suddenly mens last to the of in but what a will season army must column out rapidly brooch you a with singular were socket by is hair not wedding that harsh </s>\n",
      "<s> his door i that back as are him had wore there into bird towards </s>\n",
      "<s> and chamber into i strong the blue all </s>\n",
      "<s> and stone it my much again his said to </s>\n",
      "<s> indebted the for who perhaps eastern in little </s>\n",
      "\n",
      "\n",
      "Example for 2-gram model\n",
      "\n",
      "<s> we drive </s>\n",
      "<s> he did not apparently have been erected a happy to know nothing </s>\n",
      "<s> i failed to night </s>\n",
      "<s> but we all right cuff so that door </s>\n",
      "<s> the wound cleaned it upon their beds </s>\n",
      "\n",
      "\n",
      "Example for 3-gram model\n",
      "\n",
      "<s> at such an expenditure as they once were he whose step i hear that it was empty and a bright sun and marked with every evil passion was turned from the room in his ways no hair on his brow set his lips to my children </s>\n",
      "<s> i hear his step daughter but was interested in this way i didnt know much of what had become suddenly deranged </s>\n",
      "<s> let me see </s>\n",
      "<s> the second from dundee i answered </s>\n",
      "<s> then i fail to see a spark where all is sweetness and delicacy and harmony and there has been more nearly correct than the opium den in which i could not dream of doing it was in india he married the daughter as long as she spoke a few feet of me unpapered and uncarpeted which turned at a better fit if i had to move and there having charming manners he was young he told us a visit to brixton road egg and with a coat to his finger tips one who is picking up a long grey dressing gown and then he has his own thoughts are not very practical with your silly talk ill set the engine at work said the words the least ray of light upon the observation and for the coming of its outrages were usually preceded by a dane who acts as assistant there pushed her out of the morning </s>\n",
      "\n",
      "\n",
      "Example for 4-gram model\n",
      "\n",
      "<s> but she could not have spoken to anyone when she was out for she had been taken down the river by the early tide this morning homeward bound to savannah </s>\n",
      "<s> the firm does so when the security is good </s>\n",
      "<s> i had stooped and was scraping at this to see exactly what it was </s>\n",
      "<s> dont you see that the enemys preparations have gone so far that we cannot risk the presence of a light </s>\n",
      "<s> you have doubtless heard of the beryl coronet holmes said i this is too serious for any hesitation </s>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [1,2,3,4]:\n",
    "    print \"Example for %d-gram model\\n\"%(i)\n",
    "    for j in range(5):\n",
    "        print Generator(i)\n",
    "        if j%5 == 4:\n",
    "            print '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def Probability(sent, model):\n",
    "    sent = sent.lower()\n",
    "    word = sent.split(' ')\n",
    "    word = ['<s>'] + word + ['</s>']\n",
    "    \n",
    "    prob_sent = 0\n",
    "    try:\n",
    "        if model == 1:\n",
    "            for i,w in enumerate(word):\n",
    "                prob_sent += math.log(MLE_unigram(word[i])) #Refer MLE_ngram() above \n",
    "\n",
    "        elif model == 2:\n",
    "            for i,w in enumerate(word[:-1]):\n",
    "                prob_sent += math.log(MLE_bigram(word[i],word[i+1]))\n",
    "\n",
    "        elif model == 3:\n",
    "            for i,w in enumerate(word[:-2]):\n",
    "                prob_sent += math.log(MLE_trigram(word[i],word[i+1],word[i+2]))\n",
    "\n",
    "        elif model == 4:\n",
    "            for i,w in enumerate(word[:-3]):\n",
    "                prob_sent += math.log(MLE_quadgram(word[i],word[i+1],word[i+2],word[i+3]))\n",
    "        \n",
    "    except:\n",
    "        prob_sent = 'NA - Only for sentences in training set'\n",
    "        \n",
    "    return prob_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Probability of the sentence \"i know where it is\" in log-space\n",
      "\n",
      "For 1-gram model is = -31.781409\n",
      "For 2-gram model is = -19.051296\n",
      "For 3-gram model is = -14.144396\n",
      "For 4-gram model is = -4.094345\n",
      "\n",
      "Probability of the sentence \"then when the row broke out i had a little moist red paint in the palm of my hand\" in log-space\n",
      "\n",
      "For 1-gram model is = -125.799190\n",
      "For 2-gram model is = -64.856190\n",
      "For 3-gram model is = -34.120748\n",
      "For 4-gram model is = -6.040255\n",
      "\n",
      "Probability of the sentence \"her banker or her lawyer\" in log-space\n",
      "\n",
      "For 1-gram model is = -42.501986\n",
      "For 2-gram model is = -26.050666\n",
      "For 3-gram model is = -3.332205\n",
      "For 4-gram model is = 0.000000\n"
     ]
    }
   ],
   "source": [
    "for sent in [\"i know where it is\", 'then when the row broke out i had a little moist red paint in the palm of my hand', 'her banker or her lawyer']:\n",
    "    print '\\nProbability of the sentence \"%s\" in log-space\\n'%(sent)\n",
    "    for i in [1,2,3,4]:\n",
    "        print 'For %d-gram model is = %f'%(i,Probability('%s'%(sent), i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add one (Only for Bi-Gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddOne(a,b):\n",
    "    total_a = cfdist_uni[a]\n",
    "    total_b = cfdist_bi[a][b]\n",
    "    \n",
    "    prob = float(total_b + 1)/(total_a + count_uni)\n",
    "    eff_count = prob*float(total_a)\n",
    "    pre_count = total_b\n",
    " \n",
    "    return [MLE_bigram(a,b) ,prob , pre_count, eff_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of drastic change in Add - 1 Smoothing:\n",
      "\n",
      "['Bigram', 'Prob_w/o_AddOne', 'Prob_AddOne', 'Pre-AddOne', 'Post-AddOne']\n",
      "['is of', 0.015124555160142349, 0.0019745502413339184, 17, 2.2193944712593243]\n",
      "['he is', 0.047716428084526245, 0.00750607886668781, 70, 11.011417697431018]\n",
      "['<s> i', 0.1477255889761446, 0.06770232684349772, 997, 456.9230038667661]\n",
      "\n",
      "\n",
      "This drastic change is due to distributing the probability mass of existing bigrams to non-existing bigrams\n",
      "\n",
      "Example - For words \"of\" and \"yell\" in corpus, the bigram \"of yell\" does not exist in the corpus but after Add - 1 smoothing its effective count increases\n",
      "\n",
      "['Bigram', 'Prob_w/o_AddOne', 'Prob_AddOne', 'Pre-AddOne', 'Post-AddOne']\n",
      "['of yell', 0.0, 9.392317084624777e-05, 0, 0.24936601859678784]\n"
     ]
    }
   ],
   "source": [
    "print 'Examples of drastic change in Add - 1 Smoothing:\\n'\n",
    "\n",
    "print ['Bigram','Prob_w/o_AddOne','Prob_AddOne', 'Pre-AddOne', 'Post-AddOne']\n",
    "print ['is of'] + AddOne('is','of')\n",
    "print ['he is'] + AddOne('he','is')\n",
    "print ['<s> i'] + AddOne('<s>','i')\n",
    "print '\\n'\n",
    "print 'This drastic change is due to distributing the probability mass of existing bigrams to non-existing bigrams\\n'\n",
    "print 'Example - For words \"of\" and \"yell\" in corpus, the bigram \"of yell\" does not exist in the corpus but after Add - 1 smoothing its effective count increases\\n'\n",
    "print ['Bigram','Prob_w/o_AddOne','Prob_AddOne', 'Pre-AddOne', 'Post-AddOne']\n",
    "print ['of yell'] + AddOne('of', 'yell')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good Turing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GoodTuringTechnique(model):\n",
    "    lst = []\n",
    "\n",
    "    for sent in txt_sent:\n",
    "        words = sent.split()\n",
    "        lst = lst + list(ngrams(words, model))\n",
    "\n",
    "    lst_types = FreqDist(lst)\n",
    "\n",
    "    c_values = FreqDist(lst_types.values())\n",
    "\n",
    "    eff_c = []\n",
    "    for c in range(1,11):\n",
    "        c_new = float((c+1)*c_values[c+1])/c_values[c]\n",
    "        eff_c.append(c_new)\n",
    "\n",
    "    c_old = [float(c) for c in range(1,11)]\n",
    "    d_list = []\n",
    "    for i in range(0,10):\n",
    "        d_list.append(c_old[i] - eff_c[i])\n",
    "    \n",
    "    return d_list\n",
    "    #d = np.mean(d_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following is the list of \"d\" for c from 1 to 10\n",
      "\n",
      "For 1-gram model:\n",
      "[0.28515406162464985, 0.3824451410658307, 0.6860465116279069, 0.3944723618090453, 0.40069686411149785, -0.2681818181818185, 2.086294416243655, -0.8512396694214868, -0.24369747899159577, 3.3] \n",
      "\n",
      "For 2-gram model:\n",
      "[0.676528277214776, 0.8544719555330975, 0.9126599029554479, 1.09636517328825, 0.746724890829694, 1.7022587268993838, 0.7123745819397991, 0.2638297872340427, 0.8316831683168324, 0.8666666666666671] \n",
      "\n",
      "For 3-gram model:\n",
      "[0.8571522352786713, 1.1722426470588236, 1.2838107928047968, 1.1816770186335406, 1.0661157024793386, 1.8529411764705879, 1.5531914893617023, 2.5625, -0.137931034482758, 1.283018867924529] \n",
      "\n",
      "For 4-gram model:\n",
      "[0.9483716972583515, 1.4066780821917808, 1.5887445887445888, 1.7914110429447851, 1.9166666666666665, 0.7027027027027026, 1.5714285714285712, 4.684210526315789, -2.428571428571429, 0.375] \n",
      "\n",
      "There is no constant discounting \"d\" value for any model.\n"
     ]
    }
   ],
   "source": [
    "print 'Following is the list of \"d\" for c from 1 to 10\\n'\n",
    "for i in [1,2,3,4]:\n",
    "    print 'For %d-gram model:'%(i)\n",
    "    print GoodTuringTechnique(i),'\\n'\n",
    "\n",
    "print 'There is no constant discounting \"d\" value for any model.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity (For Bi-Gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_train = ' '.join(train)\n",
    "words_train = txt_train.split(' ')\n",
    "\n",
    "for i in words_train:\n",
    "    try:\n",
    "        words_train.remove('')\n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfdist_uni_train = FreqDist(words_train)\n",
    "count_uni = cfdist_uni_train.B()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfdist_bi_train = ConditionalFreqDist()\n",
    "for i,w in enumerate(words_train[:-1]):\n",
    "    condition = w\n",
    "    if w == '</s>':\n",
    "        continue\n",
    "    else:\n",
    "        cfdist_bi_train[condition][words_train[i+1]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_train = []\n",
    "\n",
    "for sent in train:\n",
    "    words = sent.split()\n",
    "    bi_train = bi_train + list(ngrams(words, 2))\n",
    "    \n",
    "bi_types_train = FreqDist(bi_train)\n",
    "\n",
    "c_values_train = FreqDist(bi_types_train.values())\n",
    "\n",
    "eff_c_train = []\n",
    "for c in range(1,11):\n",
    "    c_new = float((c+1)*c_values_train[c+1])/c_values_train[c]\n",
    "    eff_c_train.append(c_new)\n",
    "\n",
    "c_old_train = [float(c) for c in range(1,11)]\n",
    "d_list_train = []\n",
    "\n",
    "for i in range(0,10):\n",
    "    d_list_train.append(c_old_train[i] - eff_c_train[i])\n",
    "\n",
    "d_train = np.mean(d_list_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0603280318799744"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_train #For ease of calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the test dataset ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_test = ' '.join(test)\n",
    "words_test = txt_test.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add One - Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddOne_train(a,b):\n",
    "    total_a = cfdist_uni_train[a]\n",
    "    total_b = cfdist_bi_train[a][b]\n",
    "    \n",
    "    prob = float(total_b + 1)/(total_a + count_uni)\n",
    " \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_addone = 0\n",
    "N = 0\n",
    "for i in range(len(words_test)-1):\n",
    "    if words_test[i] != '</s>':\n",
    "        product_addone = product_addone + math.log(AddOne_train(words_test[i],words_test[i+1]))\n",
    "        N += 1\n",
    "        \n",
    "log_ppw_addone = -1*(product_addone/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppw_addone = math.exp(log_ppw_addone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good Turing - Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GoodTuring_train(a,b):\n",
    "    prob_old = cfdist_bi_train[a][b]\n",
    "    \n",
    "    if prob_old == 0:\n",
    "        prob_new = float(c_values_train[1])/bi_types_train.N()\n",
    "        \n",
    "    elif prob_old == 1:    # d_train > 1, so to avoid negative prob_new\n",
    "        prob_new = (float(cfdist_bi_train[a][b]) - d_list_train[0])/cfdist_uni_train[a]\n",
    "        \n",
    "    else:\n",
    "        prob_new = (float(cfdist_bi_train[a][b]) - d_train)/cfdist_uni_train[a]\n",
    "        \n",
    "    return prob_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_goodturing = 0\n",
    "N = 0\n",
    "for i in range(len(words_test)-1):\n",
    "    if words_test[i] != '</s>':\n",
    "        product_goodturing = product_goodturing + math.log(GoodTuring_train(words_test[i],words_test[i+1]))\n",
    "        N += 1\n",
    "        \n",
    "log_ppw_goodturing = -1*(product_goodturing/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppw_goodturing = math.exp(log_ppw_goodturing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Bi-Gram Model:\n",
      "\n",
      "Perplexity of Add - 1 = 1534.031329\n",
      "Perplexity of Good Turing = 17.757636\n",
      "\n",
      "Good Turing Smoothing performs better than Add - 1\n"
     ]
    }
   ],
   "source": [
    "print 'For Bi-Gram Model:\\n'\n",
    "print 'Perplexity of Add - 1 = %f'%(ppw_addone)\n",
    "print 'Perplexity of Good Turing = %f\\n'%(ppw_goodturing)\n",
    "\n",
    "if ppw_goodturing < ppw_addone:\n",
    "    print 'Good Turing Smoothing performs better than Add - 1'\n",
    "else:\n",
    "    print 'Add - 1 performs better than Good Turing Smoothing'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
