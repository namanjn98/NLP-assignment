{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Assignment - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Parse the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'sher.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-fc7d7cef0510>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sher.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'Text: Sherlock Holmes'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'sher.txt'"
     ]
    }
   ],
   "source": [
    "txt = open('sher.txt')\n",
    "txt = txt.read()\n",
    "print 'Text: Sherlock Holmes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = RegexpTokenizer(r'[\\w\\.\\?\\!\\'-]+') \n",
    "test = token.tokenize(txt)\n",
    "\n",
    "ban = []\n",
    "can__ = []\n",
    "can_ = []\n",
    "\n",
    "for i,w in enumerate(test):\n",
    "    if '.' in w:\n",
    "        if len(w) > 1 and w[-2] in 'IXV':\n",
    "            ban.append(i)  \n",
    "    if \"'\" in w:\n",
    "        test[i] = test[i].replace(\"'\",'')\n",
    "            \n",
    "for i,w in enumerate(test):            \n",
    "    if '--' in w:\n",
    "        test[i] = test[i].replace('--',' ')\n",
    "    elif '-' in w:\n",
    "        test[i] = test[i].replace('-',' ')\n",
    "\n",
    "t = []\n",
    "for j in range(len(test)):\n",
    "    if j in ban:\n",
    "        continue\n",
    "    else:\n",
    "        t.append(test[j])\n",
    "\n",
    "test = t\n",
    "test = ' '.join(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.lower()\n",
    "txt_sent = sent_tokenize(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(txt_sent)):\n",
    "    txt_sent[i] = '<s> '+ txt_sent[i][:-1] + ' </s>'  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = len(txt_sent)*8/10\n",
    "train = txt_sent[:index]\n",
    "test = txt_sent[index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams & MLE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.probability import MLEProbDist\n",
    "from nltk.probability import ConditionalFreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = ' '.join(txt_sent)\n",
    "words = txt.split(' ')\n",
    "\n",
    "for i in words:\n",
    "    try:\n",
    "        words.remove('')\n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unigram exists = 7989\n",
      "Number of unigrams possible = 7989\n",
      "\n",
      "Compute MLE using MLE_unigram(word)\n",
      "\n",
      "Example:\n",
      "MLE_unigram(\"the\") = 0.047360\n",
      "MLE_unigram(\"holmes\") = 0.003886\n"
     ]
    }
   ],
   "source": [
    "cfdist_uni = FreqDist(words)\n",
    "\n",
    "def MLE_unigram(a):\n",
    "    k = MLEProbDist(cfdist_uni)\n",
    "    return k.prob(a)\n",
    "\n",
    "count_uni = cfdist_uni.B()\n",
    "print \"Number of unigram exists = %d\"%(count_uni)\n",
    "print \"Number of unigrams possible = %d\"%(count_uni)\n",
    "\n",
    "print '\\nCompute MLE using MLE_unigram(word)\\n'\n",
    "print 'Example:'\n",
    "print 'MLE_unigram(\"the\") = %f'%(MLE_unigram(\"the\"))\n",
    "print 'MLE_unigram(\"holmes\") = %f'%(MLE_unigram(\"holmes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bigram exists = 49316\n",
      "Number of bigrams possible = 31904072\n",
      "\n",
      "Compute MLE using MLE_bigram(word)\n",
      "\n",
      "Example:\n",
      "MLE_bigram(\"is\",\"the\") = 0.065836\n",
      "MLE_bigram(\"said\",\"holmes\") = 0.226337\n"
     ]
    }
   ],
   "source": [
    "cfdist_bi = ConditionalFreqDist()\n",
    "for i,w in enumerate(words[:-1]):\n",
    "    condition = w\n",
    "    if w == '</s>':\n",
    "        continue\n",
    "    else:\n",
    "        cfdist_bi[condition][words[i+1]] += 1\n",
    "\n",
    "def MLE_bigram(a,b):\n",
    "    k = MLEProbDist(cfdist_bi[a])\n",
    "    return k.prob(b)\n",
    "\n",
    "count_bi=0\n",
    "for key in cfdist_bi:\n",
    "    count_bi += cfdist_bi[key].B()\n",
    "\n",
    "print \"Number of bigram exists = %d\"%(count_bi)\n",
    "\n",
    "all_count_bi = (count_uni/2) * (count_uni - 1)\n",
    "print \"Number of bigrams possible = %d\"%(all_count_bi)\n",
    "\n",
    "print '\\nCompute MLE using MLE_bigram(word)\\n'\n",
    "print 'Example:'\n",
    "print 'MLE_bigram(\"is\",\"the\") = %f'%(MLE_bigram(\"is\",\"the\"))\n",
    "print 'MLE_bigram(\"said\",\"holmes\") = %f'%(MLE_bigram(\"said\",\"holmes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trigram exists = 87832\n",
      "Number of trigrams possible = 84918007636\n",
      "\n",
      "Compute MLE using MLE_trigram(word)\n",
      "\n",
      "Example:\n",
      "MLE_trigram(\"i\",\"have\",\"a\") = 0.043189\n",
      "MLE_trigram(\"go\",\"for\",\"it\") = 0.200000\n"
     ]
    }
   ],
   "source": [
    "cfdist_tri = ConditionalFreqDist()\n",
    "for i,w in enumerate(words[:-2]):\n",
    "    condition = words[i] +' ' + words[i+1]\n",
    "    if '</s>' in condition:\n",
    "        continue\n",
    "    else:\n",
    "        cfdist_tri[condition][words[i+2]] += 1\n",
    "\n",
    "def MLE_trigram(a,b,c):\n",
    "    k = MLEProbDist(cfdist_tri[a+\" \"+b])\n",
    "    return k.prob(c)\n",
    "\n",
    "count_tri=0\n",
    "for key in cfdist_tri:\n",
    "    count_tri += cfdist_tri[key].B()\n",
    "\n",
    "print \"Number of trigram exists = %d\"%(count_tri)\n",
    "\n",
    "all_count_tri = (count_uni/6) * (count_uni - 1) *  (count_uni - 2)\n",
    "print \"Number of trigrams possible = %d\"%(all_count_tri)\n",
    "\n",
    "print '\\nCompute MLE using MLE_trigram(word)\\n'\n",
    "print 'Example:'\n",
    "print 'MLE_trigram(\"i\",\"have\",\"a\") = %f'%(MLE_trigram(\"i\",\"have\",\"a\"))\n",
    "print 'MLE_trigram(\"go\",\"for\",\"it\") = %f'%(MLE_trigram(\"go\",\"for\",\"it\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of quadgram exists = 107085\n",
      "Number of quadgrams possible = 169156671210912\n",
      "\n",
      "Compute MLE using MLE_quadgram(word)\n",
      "\n",
      "Example:\n",
      "MLE_quadgram(\"i\",\"know\",\"that\",\"i\") = 0.200000\n",
      "MLE_quadgram(\"he\",\"drew\",\"out\",\"a\") = 1.000000\n"
     ]
    }
   ],
   "source": [
    "cfdist_quad = ConditionalFreqDist()\n",
    "for i,w in enumerate(words[:-3]):\n",
    "    condition = words[i] +' ' + words[i+1] + ' ' + words[i+2]\n",
    "    if '</s>' in condition:\n",
    "        continue\n",
    "    else:\n",
    "        cfdist_quad[condition][words[i+3]] += 1\n",
    "\n",
    "def MLE_quadgram(a,b,c,d):\n",
    "    k = MLEProbDist(cfdist_quad[a+\" \"+b+\" \"+c])\n",
    "    return k.prob(d)\n",
    "\n",
    "count_quad = 0\n",
    "for key in cfdist_quad:\n",
    "    count_quad += cfdist_quad[key].B()\n",
    "\n",
    "print \"Number of quadgram exists = %d\"%(count_quad)\n",
    "\n",
    "all_count_quad = (count_uni/24) * (count_uni - 1) * (count_uni - 2) *  (count_uni - 3)\n",
    "print \"Number of quadgrams possible = %d\"%(all_count_quad)\n",
    "\n",
    "print '\\nCompute MLE using MLE_quadgram(word)\\n'\n",
    "print 'Example:'\n",
    "print 'MLE_quadgram(\"i\",\"know\",\"that\",\"i\") = %f'%(MLE_quadgram(\"i\",\"know\",\"that\",\"i\"))\n",
    "print 'MLE_quadgram(\"he\",\"drew\",\"out\",\"a\") = %f'%(MLE_quadgram(\"he\",\"drew\",\"out\",\"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator(model):       #input is a model from [1,2,3,4]\n",
    "    if model == 1:         \n",
    "        sent = ['<s>']\n",
    "        i = 0\n",
    "        while i == 0:\n",
    "            arr = cfdist_uni.keys()\n",
    "            prob = [MLE_unigram(key) for key in arr]\n",
    "            \n",
    "            gen_word = np.random.choice(arr,1,p = prob)   #randomly selecting words with probability\n",
    "            \n",
    "            if gen_word[0] != '<s>':      #Avoiding '<s>' words\n",
    "                sent.append(gen_word[0])\n",
    "                \n",
    "            if gen_word == '</s>':\n",
    "                i = 1\n",
    "        \n",
    "    elif model == 2:\n",
    "        sent = ['<s>']\n",
    "        i = 0\n",
    "        while i == 0:\n",
    "            arr = cfdist_bi[sent[-1]].keys()\n",
    "            prob = [MLE_bigram(sent[-1],key) for key in arr]\n",
    "            gen_word = np.random.choice(arr,1,p = prob)\n",
    "            sent.append(gen_word[0])\n",
    "            if gen_word == '</s>':\n",
    "                i = 1\n",
    "        \n",
    "    elif model == 3:\n",
    "        sent = ['<s>']      #First word after '<s>' can be selected via bi-gram as P(<s>,<s>,FW) == P(<s>,FW)\n",
    "        \n",
    "        arr = cfdist_bi[sent[-1]].keys()\n",
    "        prob = [MLE_bigram(sent[-1],key) for key in arr]\n",
    "        gen_word = np.random.choice(arr,1,p = prob)\n",
    "        sent.append(gen_word[0])\n",
    "        \n",
    "        i = 0\n",
    "        while i == 0:\n",
    "            arr = cfdist_tri[sent[-2] +' '+ sent[-1]].keys()\n",
    "            prob = [MLE_trigram(sent[-2],sent[-1],key) for key in arr]\n",
    "            gen_word = np.random.choice(arr,1,p = prob)\n",
    "            sent.append(gen_word[0])\n",
    "            if gen_word == '</s>':\n",
    "                i = 1\n",
    "        \n",
    "    elif model == 4:\n",
    "        sent = ['<s>']      #First word after '<s>' can be selected via bi-gram as P(<s>,<s>,<s>,FW) == P(<s>,FW)\n",
    "        \n",
    "        arr = cfdist_bi[sent[-1]].keys()\n",
    "        prob = [MLE_bigram(sent[-1],key) for key in arr]\n",
    "        gen_word = np.random.choice(arr,1,p = prob)\n",
    "        sent.append(gen_word[0])\n",
    "        \n",
    "        i = 0\n",
    "        while i == 0:       #Second word after '<s>' can be selected via tri-gram as P(<s>,<s>,FW,SW) == P(<s>,FW,SW)\n",
    "            arr = cfdist_tri[sent[-2] +' '+ sent[-1]].keys()\n",
    "            prob = [MLE_trigram(sent[-2],sent[-1],key) for key in arr]\n",
    "            gen_word = np.random.choice(arr,1,p = prob)\n",
    "            i = 1\n",
    "            \n",
    "            if '</s>' in gen_word[0]:\n",
    "                i = 0\n",
    "            else:\n",
    "                sent.append(gen_word[0])\n",
    "        \n",
    "        i = 0\n",
    "        while i == 0:\n",
    "            arr = cfdist_quad[sent[-3]+' '+sent[-2] +' '+ sent[-1]].keys()\n",
    "            prob = [MLE_quadgram(sent[-3],sent[-2],sent[-1],key) for key in arr]\n",
    "            gen_word = np.random.choice(arr,1,p = prob)\n",
    "            sent.append(gen_word[0])\n",
    "            if gen_word == '</s>':\n",
    "                i = 1\n",
    "    \n",
    "    sent = ' '.join(sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example for 1-gram model\n",
      "\n",
      "<s> and stretch over go terror i bed down tide that did frenzy a he </s>\n",
      "<s> crate i nights very the jack night what i me hundred himself </s>\n",
      "<s> your at would turners neat </s>\n",
      "<s> of papers have and to is across was that he have to ulster its had was until little too the i path evening eyes what left than he that well in gone </s>\n",
      "<s> you st. of she me gainer silence not is told breath also opinion friend and is we american perhaps robberies other be looking </s>\n",
      "\n",
      "\n",
      "Example for 2-gram model\n",
      "\n",
      "<s> but it and sister was and tucked his arms round to </s>\n",
      "<s> my hand was on the laughing </s>\n",
      "<s> but we have not know </s>\n",
      "<s> but only what i cannot tell you will excuse my attention since then it seldom was never got if on the page he had a character has guessed as we had sat after all those of barbaric opulence which is anything of the left glove and finally of holder that my chair </s>\n",
      "<s> this not be you understand said holmes succinct description tallied in bohemia in my friend whom you an actress myself in the clank of several other characteristics with your jacket and i will simplify matters immensely indebted to assure you go back at the details and alternating from the chain we turned to me </s>\n",
      "\n",
      "\n",
      "Example for 3-gram model\n",
      "\n",
      "<s> here he comes if i have just been there and he glared at the opium den and we were to have been able to see that the posterior third of the old days in victoria </s>\n",
      "<s> i know that </s>\n",
      "<s> if you are nearing the end wall </s>\n",
      "<s> the boscombe pool and that it was crushed in the disappearance of the sole in order to get to him to night at a moment later we saw dr. grimesby roylotts death and i would pay ten </s>\n",
      "<s> i had betrayed myself i began to laugh at me with your company said sherlock holmes said mr. merryweather who is the person whom mccarthy expected to take no notice </s>\n",
      "\n",
      "\n",
      "Example for 4-gram model\n",
      "\n",
      "<s> it is a great thing for me to judge you said holmes as the old man solemnly </s>\n",
      "<s> who was the man and who was it brought him the coronet </s>\n",
      "<s> he drew out a lens and a forceps lying upon the table </s>\n",
      "<s> i daresay that if i had married lord st. simon shook his head </s>\n",
      "<s> i know that i am tempted to give some account of it in that press </s>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [1,2,3,4]:\n",
    "    print \"Example for %d-gram model\\n\"%(i)\n",
    "    for j in range(5):\n",
    "        print Generator(i)\n",
    "        if j%5 == 4:\n",
    "            print '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def Probability(sent, model):\n",
    "    sent = sent.lower()\n",
    "    word = sent.split(' ')\n",
    "    word = ['<s>'] + word + ['</s>']\n",
    "    \n",
    "    prob_sent = 0\n",
    "    try:\n",
    "        if model == 1:\n",
    "            for i,w in enumerate(word):\n",
    "                prob_sent += math.log(MLE_unigram(word[i])) #Refer MLE_ngram() above \n",
    "\n",
    "        elif model == 2:\n",
    "            for i,w in enumerate(word[:-1]):\n",
    "                prob_sent += math.log(MLE_bigram(word[i],word[i+1]))\n",
    "\n",
    "        elif model == 3:\n",
    "            for i,w in enumerate(word[:-2]):\n",
    "                prob_sent += math.log(MLE_trigram(word[i],word[i+1],word[i+2]))\n",
    "\n",
    "        elif model == 4:\n",
    "            for i,w in enumerate(word[:-3]):\n",
    "                prob_sent += math.log(MLE_quadgram(word[i],word[i+1],word[i+2],word[i+3]))\n",
    "        \n",
    "    except:\n",
    "        prob_sent = 'NA - Only for sentences in training set'\n",
    "        \n",
    "    return prob_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Probability of the sentence \"i know where it is\" in log-space\n",
      "\n",
      "For 1-gram model is = -31.781409\n",
      "For 2-gram model is = -19.051296\n",
      "For 3-gram model is = -14.144396\n",
      "For 4-gram model is = -4.094345\n",
      "\n",
      "Probability of the sentence \"then when the row broke out i had a little moist red paint in the palm of my hand\" in log-space\n",
      "\n",
      "For 1-gram model is = -125.799190\n",
      "For 2-gram model is = -64.856190\n",
      "For 3-gram model is = -34.120748\n",
      "For 4-gram model is = -6.040255\n",
      "\n",
      "Probability of the sentence \"her banker or her lawyer\" in log-space\n",
      "\n",
      "For 1-gram model is = -42.501986\n",
      "For 2-gram model is = -26.050666\n",
      "For 3-gram model is = -3.332205\n",
      "For 4-gram model is = 0.000000\n"
     ]
    }
   ],
   "source": [
    "for sent in [\"i know where it is\", 'then when the row broke out i had a little moist red paint in the palm of my hand', 'her banker or her lawyer']:\n",
    "    print '\\nProbability of the sentence \"%s\" in log-space\\n'%(sent)\n",
    "    for i in [1,2,3,4]:\n",
    "        print 'For %d-gram model is = %f'%(i,Probability('%s'%(sent), i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add one (Only for Bi-Gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddOne(a,b):\n",
    "    total_a = cfdist_uni[a]\n",
    "    total_b = cfdist_bi[a][b]\n",
    "    \n",
    "    prob = float(total_b + 1)/(total_a + count_uni)\n",
    "    eff_count = prob*float(total_a)\n",
    "    pre_count = total_b\n",
    " \n",
    "    return [MLE_bigram(a,b) ,prob , pre_count, eff_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of drastic change in Add - 1 Smoothing:\n",
      "\n",
      "['Bigram', 'Prob_w/o_AddOne', 'Prob_AddOne', 'Pre-AddOne', 'Post-AddOne']\n",
      "['is of', 0.015124555160142349, 0.0019745502413339184, 17, 2.2193944712593243]\n",
      "['he is', 0.047716428084526245, 0.00750607886668781, 70, 11.011417697431018]\n",
      "['<s> i', 0.1477255889761446, 0.06770232684349772, 997, 456.9230038667661]\n",
      "\n",
      "\n",
      "This drastic change is due to distributing the probability mass of existing bigrams to non-existing bigrams\n",
      "\n",
      "Example - For words \"of\" and \"yell\" in corpus, the bigram \"of yell\" does not exist in the corpus but after Add - 1 smoothing its effective count increases\n",
      "\n",
      "['Bigram', 'Prob_w/o_AddOne', 'Prob_AddOne', 'Pre-AddOne', 'Post-AddOne']\n",
      "['of yell', 0.0, 9.392317084624777e-05, 0, 0.24936601859678784]\n"
     ]
    }
   ],
   "source": [
    "print 'Examples of drastic change in Add - 1 Smoothing:\\n'\n",
    "\n",
    "print ['Bigram','Prob_w/o_AddOne','Prob_AddOne', 'Pre-AddOne', 'Post-AddOne']\n",
    "print ['is of'] + AddOne('is','of')\n",
    "print ['he is'] + AddOne('he','is')\n",
    "print ['<s> i'] + AddOne('<s>','i')\n",
    "print '\\n'\n",
    "print 'This drastic change is due to distributing the probability mass of existing bigrams to non-existing bigrams\\n'\n",
    "print 'Example - For words \"of\" and \"yell\" in corpus, the bigram \"of yell\" does not exist in the corpus but after Add - 1 smoothing its effective count increases\\n'\n",
    "print ['Bigram','Prob_w/o_AddOne','Prob_AddOne', 'Pre-AddOne', 'Post-AddOne']\n",
    "print ['of yell'] + AddOne('of', 'yell')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good Turing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GoodTuringTechnique(model):\n",
    "    lst = []\n",
    "\n",
    "    for sent in txt_sent:\n",
    "        words = sent.split()\n",
    "        lst = lst + list(ngrams(words, model))\n",
    "\n",
    "    lst_types = FreqDist(lst)\n",
    "\n",
    "    c_values = FreqDist(lst_types.values())\n",
    "\n",
    "    eff_c = []\n",
    "    for c in range(1,11):\n",
    "        c_new = float((c+1)*c_values[c+1])/c_values[c]\n",
    "        eff_c.append(c_new)\n",
    "\n",
    "    c_old = [float(c) for c in range(1,11)]\n",
    "    d_list = []\n",
    "    for i in range(0,10):\n",
    "        d_list.append(c_old[i] - eff_c[i])\n",
    "    \n",
    "    return d_list\n",
    "    #d = np.mean(d_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following is the list of \"d\" for c from 1 to 10\n",
      "\n",
      "For 1-gram model:\n",
      "[0.28515406162464985, 0.3824451410658307, 0.6860465116279069, 0.3944723618090453, 0.40069686411149785, -0.2681818181818185, 2.086294416243655, -0.8512396694214868, -0.24369747899159577, 3.3] \n",
      "\n",
      "For 2-gram model:\n",
      "[0.676528277214776, 0.8544719555330975, 0.9126599029554479, 1.09636517328825, 0.746724890829694, 1.7022587268993838, 0.7123745819397991, 0.2638297872340427, 0.8316831683168324, 0.8666666666666671] \n",
      "\n",
      "For 3-gram model:\n",
      "[0.8571522352786713, 1.1722426470588236, 1.2838107928047968, 1.1816770186335406, 1.0661157024793386, 1.8529411764705879, 1.5531914893617023, 2.5625, -0.137931034482758, 1.283018867924529] \n",
      "\n",
      "For 4-gram model:\n",
      "[0.9483716972583515, 1.4066780821917808, 1.5887445887445888, 1.7914110429447851, 1.9166666666666665, 0.7027027027027026, 1.5714285714285712, 4.684210526315789, -2.428571428571429, 0.375] \n",
      "\n",
      "There is no constant discounting \"d\" value for any model.\n"
     ]
    }
   ],
   "source": [
    "print 'Following is the list of \"d\" for c from 1 to 10\\n'\n",
    "for i in [1,2,3,4]:\n",
    "    print 'For %d-gram model:'%(i)\n",
    "    print GoodTuringTechnique(i),'\\n'\n",
    "\n",
    "print 'There is no constant discounting \"d\" value for any model.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity (For Bi-Gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_train = ' '.join(train)\n",
    "words_train = txt_train.split(' ')\n",
    "\n",
    "for i in words_train:\n",
    "    try:\n",
    "        words_train.remove('')\n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfdist_uni_train = FreqDist(words_train)\n",
    "count_uni = cfdist_uni_train.B()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfdist_bi_train = ConditionalFreqDist()\n",
    "for i,w in enumerate(words_train[:-1]):\n",
    "    condition = w\n",
    "    if w == '</s>':\n",
    "        continue\n",
    "    else:\n",
    "        cfdist_bi_train[condition][words_train[i+1]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_train = []\n",
    "\n",
    "for sent in train:\n",
    "    words = sent.split()\n",
    "    bi_train = bi_train + list(ngrams(words, 2))\n",
    "    \n",
    "bi_types_train = FreqDist(bi_train)\n",
    "\n",
    "c_values_train = FreqDist(bi_types_train.values())\n",
    "\n",
    "eff_c_train = []\n",
    "for c in range(1,11):\n",
    "    c_new = float((c+1)*c_values_train[c+1])/c_values_train[c]\n",
    "    eff_c_train.append(c_new)\n",
    "\n",
    "c_old_train = [float(c) for c in range(1,11)]\n",
    "d_list_train = []\n",
    "\n",
    "for i in range(0,10):\n",
    "    d_list_train.append(c_old_train[i] - eff_c_train[i])\n",
    "\n",
    "d_train = np.mean(d_list_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0603280318799744"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_train #For ease of calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the test dataset ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_test = ' '.join(test)\n",
    "words_test = txt_test.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add One - Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddOne_train(a,b):\n",
    "    total_a = cfdist_uni_train[a]\n",
    "    total_b = cfdist_bi_train[a][b]\n",
    "    \n",
    "    prob = float(total_b + 1)/(total_a + count_uni)\n",
    " \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_addone = 0\n",
    "N = 0\n",
    "for i in range(len(words_test)-1):\n",
    "    if words_test[i] != '</s>':\n",
    "        product_addone = product_addone + math.log(AddOne_train(words_test[i],words_test[i+1]))\n",
    "        N += 1\n",
    "        \n",
    "log_ppw_addone = -1*(product_addone/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppw_addone = math.exp(log_ppw_addone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good Turing - Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GoodTuring_train(a,b):\n",
    "    prob_old = cfdist_bi_train[a][b]\n",
    "    \n",
    "    if prob_old == 0:\n",
    "        prob_new = float(c_values_train[1])/bi_types_train.N()\n",
    "        \n",
    "    elif prob_old == 1:    # d_train > 1, so to avoid negative prob_new\n",
    "        prob_new = (float(cfdist_bi_train[a][b]) - d_list_train[0])/cfdist_uni_train[a]\n",
    "        \n",
    "    else:\n",
    "        prob_new = (float(cfdist_bi_train[a][b]) - d_train)/cfdist_uni_train[a]\n",
    "        \n",
    "    return prob_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_goodturing = 0\n",
    "N = 0\n",
    "for i in range(len(words_test)-1):\n",
    "    if words_test[i] != '</s>':\n",
    "        product_goodturing = product_goodturing + math.log(GoodTuring_train(words_test[i],words_test[i+1]))\n",
    "        N += 1\n",
    "        \n",
    "log_ppw_goodturing = -1*(product_goodturing/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppw_goodturing = math.exp(log_ppw_goodturing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Bi-Gram Model:\n",
      "\n",
      "Perplexity of Add - 1 = 1534.031329\n",
      "Perplexity of Good Turing = 17.757636\n",
      "\n",
      "Good Turing Smoothing performs better than Add - 1\n"
     ]
    }
   ],
   "source": [
    "print 'For Bi-Gram Model:\\n'\n",
    "print 'Perplexity of Add - 1 = %f'%(ppw_addone)\n",
    "print 'Perplexity of Good Turing = %f\\n'%(ppw_goodturing)\n",
    "\n",
    "if ppw_goodturing < ppw_addone:\n",
    "    print 'Good Turing Smoothing performs better than Add - 1'\n",
    "else:\n",
    "    print 'Add - 1 performs better than Good Turing Smoothing'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
